[
  {"id":"844354","author":[{"family":"Parasuraman","given":"R."},{"family":"Sheridan","given":"T.B."},{"family":"Wickens","given":"C.D."}],"citation-key":"844354","container-title":"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","DOI":"10.1109/3468.844354","issue":"3","issued":{"date-parts":[["2000"]]},"page":"286-297","title":"A model for types and levels of human interaction with automation","type":"article-journal","volume":"30"},
  {"id":"9241-210","author":[{"literal":"DIN EN ISO 9241-210"}],"citation-key":"9241-210","DOI":"10.31030/1728173","issued":{"date-parts":[["2020",1]]},"number":"DIN EN ISO 9241-210:2011-01","publisher":"Beuth Verlag GmbH","title":"DIN EN ISO 9241-210:2011-01, ergonomie der mensch-system-interaktion - teil 210: Prozess zur gestaltung gebrauchstauglicher interaktiver systeme (ISO 9241-210:2010); deutsche fassung EN ISO 9241-210:2010","type":"standard","URL":"https://doi.org/10.31030/1728173"},
  {"id":"BAINBRIDGE1983775","abstract":"This paper discusses the ways in which automation of industrial processes may expand rather than eliminate problems with the human operator. Some comments will be made on methods of alleviating these problems within the ‘classic’ approach of leaving the operator with responsibility for abnormal conditions, and on the potential for continued use of the human operator for on-line decision-making within human-computer collaboration.","author":[{"family":"Bainbridge","given":"Lisanne"}],"citation-key":"BAINBRIDGE1983775","container-title":"Automatica","DOI":"https://doi.org/10.1016/0005-1098(83)90046-8","ISSN":"0005-1098","issue":"6","issued":{"date-parts":[["1983"]]},"page":"775-779","title":"Ironies of automation","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/0005109883900468","volume":"19"},
  {"id":"doi:10.1177/0018720816644364","abstract":"Objective: The current status of human–robot interaction (HRI) is reviewed, and key current research challenges for the human factors community are described.Background: Robots have evolved from continuous human-controlled master–slave servomechanisms for handling nuclear waste to a broad range of robots incorporating artificial intelligence for many applications and under human supervisory control.Methods: This mini-review describes HRI developments in four application areas and what are the challenges for human factors research.Results: In addition to a plethora of research papers, evidence of success is manifest in live demonstrations of robot capability under various forms of human control.Conclusions: HRI is a rapidly evolving field. Specialized robots under human teleoperation have proven successful in hazardous environments and medical application, as have specialized telerobots under human supervisory control for space and repetitive industrial tasks. Research in areas of self-driving cars, intimate collaboration with humans in manipulation tasks, human control of humanoid robots for hazardous environments, and social interaction with robots is at initial stages. The efficacy of humanoid general-purpose robots has yet to be proven.Applications: HRI is now applied in almost all robot tasks, including manufacturing, space, aviation, undersea, surgery, rehabilitation, agriculture, education, package fetch and delivery, policing, and military operations.","author":[{"family":"Sheridan","given":"Thomas B."}],"citation-key":"doi:10.1177/0018720816644364","container-title":"Human Factors","DOI":"10.1177/0018720816644364","issue":"4","issued":{"date-parts":[["2016"]]},"page":"525-532","title":"Human–robot interaction: Status and challenges","type":"article-journal","URL":"https://doi.org/10.1177/0018720816644364","volume":"58"},
  {"id":"doi:10.1177/0018720816681350","abstract":"As autonomous and semiautonomous systems are developed for automotive, aviation, cyber, robotics and other applications, the ability of human operators to effectively oversee and interact with them when needed poses a significant challenge. An automation conundrum exists in which as more autonomy is added to a system, and its reliability and robustness increase, the lower the situation awareness of human operators and the less likely that they will be able to take over manual control when needed. The human–autonomy systems oversight model integrates several decades of relevant autonomy research on operator situation awareness, out-of-the-loop performance problems, monitoring, and trust, which are all major challenges underlying the automation conundrum. Key design interventions for improving human performance in interacting with autonomous systems are integrated in the model, including human–automation interface features and central automation interaction paradigms comprising levels of automation, adaptive automation, and granularity of control approaches. Recommendations for the design of human–autonomy interfaces are presented and directions for future research discussed.","author":[{"family":"Endsley","given":"Mica R."}],"citation-key":"doi:10.1177/0018720816681350","container-title":"Human Factors","DOI":"10.1177/0018720816681350","issue":"1","issued":{"date-parts":[["2017"]]},"page":"5-27","title":"From here to autonomy: Lessons learned from human–automation research","type":"article-journal","URL":"https://doi.org/10.1177/0018720816681350","volume":"59"},
  {"id":"doi:10.1518/001872095779049516","abstract":"New technology is flexible in the sense that it provides practitioners with a large number of functions and options for carrying out a given task under different circumstances. However, this flexibility has a price. Because the human supervisor must select the mode best suited to a particular situation, he or she must know more than before about system operations and the operation of the system as well as satisfy new monitoring and attentional demands to track which mode the automation is in and what it is doing to manage the underlying processes. When designers proliferate modes without supporting these new cognitive demands, new mode-related error forms and failure paths can result. Mode error has been discussed in human-computer interaction for some time; however, the increased capabilities and the high level of autonomy of new automated systems appear to have created new types of mode-related problems. We explore these new aspects based on results from our own and related studies of human-automation interaction. In particular, we draw on empirical data from a series of studies of pilot-automation interaction in commercial glass cockpit aircraft to illustrate the nature, circumstances, and potential consequences of mode awareness problems in supervisory control of automated resources. The result is an expanded view of mode error that takes into account the new demands imposed by more automated systems.","author":[{"family":"Sarter","given":"Nadine B."},{"family":"Woods","given":"David D."}],"citation-key":"doi:10.1518/001872095779049516","container-title":"Human Factors","DOI":"10.1518/001872095779049516","issue":"1","issued":{"date-parts":[["1995"]]},"page":"5-19","title":"How in the world did we ever get into that mode? Mode error and awareness in supervisory control","type":"article-journal","URL":"https://doi.org/10.1518/001872095779049516","volume":"37"},
  {"id":"doi:10.1518/001872095779049543","abstract":"This paper presents a theoretical model of situation awareness based on its role in dynamic human decision making in a variety of domains. Situation awareness is presented as a predominant concern in system operation, based on a descriptive view of decision making. The relationship between situation awareness and numerous individual and environmental factors is explored. Among these factors, attention and working memory are presented as critical factors limiting operators from acquiring and interpreting information from the environment to form situation awareness, and mental models and goal-directed behavior are hypothesized as important mechanisms for overcoming these limits. The impact of design features, workload, stress, system complexity, and automation on operator situation awareness is addressed, and a taxonomy of errors in situation awareness is introduced, based on the model presented. The model is used to generate design implications for enhancing operator situation awareness and future directions for situation awareness research.","author":[{"family":"Endsley","given":"Mica R."}],"citation-key":"doi:10.1518/001872095779049543","container-title":"Human Factors","DOI":"10.1518/001872095779049543","issue":"1","issued":{"date-parts":[["1995"]]},"page":"32-64","title":"Toward a theory of situation awareness in dynamic systems","type":"article-journal","URL":"https://doi.org/10.1518/001872095779049543","volume":"37"},
  {"id":"EU_AIAct_2024","author":[{"literal":"European Parliament"},{"literal":"Council of the European Union"}],"citation-key":"EU_AIAct_2024","issued":{"date-parts":[["2024"]]},"title":"Regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (AI Act)","type":"document","URL":"https://eur-lex.europa.eu/eli/reg/2024/1689/oj/eng"},
  {"id":"LeeSee2004","author":[{"family":"Lee","given":"John D."},{"family":"See","given":"Katrina A."}],"citation-key":"LeeSee2004","container-title":"Human Factors","DOI":"10.1518/hfes.46.1.50_30392","ISSN":"0018-7208","issue":"1","issued":{"date-parts":[["2004"]]},"page":"50–80","PMID":"15151155","title":"Trust in automation: designing for appropriate reliance","type":"article-journal","URL":"https://doi.org/10.1518/hfes.46.1.50_30392","volume":"46"},
  {"id":"MILLER20191","abstract":"There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.","author":[{"family":"Miller","given":"Tim"}],"citation-key":"MILLER20191","container-title":"Artificial Intelligence","DOI":"https://doi.org/10.1016/j.artint.2018.07.007","ISSN":"0004-3702","issued":{"date-parts":[["2019"]]},"page":"1-38","title":"Explanation in artificial intelligence: Insights from the social sciences","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0004370218305988","volume":"267"},
  {"id":"Nielsen1994","author":[{"family":"Nielsen","given":"Jakob"}],"citation-key":"Nielsen1994","event-place":"San Francisco, CA","ISBN":"978-0-12-518406-9","issued":{"date-parts":[["1994"]]},"publisher":"Morgan Kaufmann","publisher-place":"San Francisco, CA","title":"Usability engineering","type":"book"},
  {"id":"Norman2013","author":[{"family":"Norman","given":"D. A."}],"citation-key":"Norman2013","ISBN":"978-0-465-05065-9","issued":{"date-parts":[["2013"]]},"publisher":"Basic Books","title":"The design of everyday things: Revised and expanded edition","type":"book"},
  {"id":"Vicente1999","author":[{"family":"Vicente","given":"Kim J."}],"citation-key":"Vicente1999","event-place":"Boca Raton, FL","ISBN":"978-0805823977","issued":{"date-parts":[["1999"]]},"publisher":"CRC Press","publisher-place":"Boca Raton, FL","title":"Cognitive work analysis: Toward safe, productive, and healthy computer-based work","type":"book"},
  {"id":"Wickens2021","author":[{"family":"Wickens","given":"Christopher D."},{"family":"Helton","given":"William S."},{"family":"Hollands","given":"Justin G."},{"family":"Banbury","given":"Simon"}],"citation-key":"Wickens2021","DOI":"10.4324/9781003177616","edition":"5","event-place":"New York, NY","ISBN":"978-1-003-17761-6","issued":{"date-parts":[["2021"]]},"publisher":"Routledge","publisher-place":"New York, NY","title":"Engineering psychology and human performance","type":"book","URL":"https://doi.org/10.4324/9781003177616"},
  {"id":"winfieldEthicalGovernanceEssential2018","abstract":"This paper explores the question of ethical governance for robotics and artificial intelligence (AI) systems. We outline a roadmap—which links a number of elements, including ethics, standards, regulation, responsible research and innovation, and public engagement—as a framework to guide ethical governance in robotics and AI. We argue that ethical governance is essential to building public trust in robotics and AI, and conclude by proposing five pillars of good ethical governance.\nThis article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.","accessed":{"date-parts":[["2025",10,17]]},"author":[{"family":"Winfield","given":"Alan F. T."},{"family":"Jirotka","given":"Marina"}],"citation-key":"winfieldEthicalGovernanceEssential2018","container-title":"Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences","DOI":"10.1098/rsta.2018.0085","issue":"2133","issued":{"date-parts":[["2018",10,15]]},"page":"20180085","publisher":"Royal Society","source":"royalsocietypublishing.org (Atypon)","title":"Ethical governance is essential to building trust in robotics and artificial intelligence systems","type":"article-journal","URL":"https://royalsocietypublishing.org/doi/10.1098/rsta.2018.0085","volume":"376"}
]
