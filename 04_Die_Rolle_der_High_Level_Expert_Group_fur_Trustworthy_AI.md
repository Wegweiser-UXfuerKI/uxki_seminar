
### Die High-Level Expert Group on Artificial Intelligence und ihre Ziele

Die High-Level Expert Group on Artificial Intelligence (HLEG) wurde im Juni 2018 von der Europäischen Kommission ins Leben gerufen. Diese unabhängige Expertengruppe besteht aus einer vielfältigen Auswahl von Fachleuten aus Wissenschaft, Industrie und Zivilgesellschaft. Ihr Hauptziel ist es, die Entwicklung und Implementierung von Künstlicher Intelligenz (KI) in Europa zu fördern und sicherzustellen, dass diese Technologien im Einklang mit europäischen Werten und Grundrechten stehen (European Commission, 2020a).

Die HLEG verfolgt mehrere wesentliche Ziele. Erstens soll durch die Definition von Anforderungen für vertrauenswürdige KI das Vertrauen in diese Technologien gestärkt werden. Dies ist besonders wichtig, da die Akzeptanz von KI in der Gesellschaft davon abhängt, dass die Menschen den Systemen vertrauen und sich sicher fühlen. Zweitens sollen ethische Prinzipien und Grundrechte geschützt werden. Dies bedeutet, dass KI-Systeme nicht nur technisch einwandfrei sein müssen, sondern auch moralische und ethische Standards einhalten müssen. Drittens soll die technische Exzellenz gefördert werden. KI-Systeme müssen robust und sicher sein, um Risiken und potenzielle Schäden zu minimieren. Viertens soll die Wettbewerbsfähigkeit europäischer Unternehmen gestärkt werden. Durch die Entwicklung innovativer und vertrauenswürdiger KI-Lösungen können sich europäische Unternehmen im globalen Wettbewerb behaupten (European Commission, 2020b).

Ein zentrales Dokument der HLEG sind die „Ethics Guidelines for Trustworthy AI“, die im April 2019 veröffentlicht wurden. Diese Leitlinien definieren, was vertrauenswürdige KI ausmacht und welche Anforderungen an solche Systeme gestellt werden sollten. Vertrauenswürdige KI basiert auf drei Hauptkomponenten: Gesetzeskonformität, Ethik und Robustheit. Gesetzeskonformität bedeutet, dass KI-Systeme alle relevanten Gesetze und Vorschriften einhalten müssen. Ethik umfasst die Respektierung ethischer Prinzipien und Werte, während Robustheit sicherstellt, dass die Systeme technisch und sozial robust sind, um unabsichtliche Schäden zu vermeiden (High-Level Expert Group on Artificial Intelligence, 2019).

### Die sieben Anforderungen der HLEG

Um diese Ziele zu erreichen, hat die HLEG sieben zentrale Anforderungen definiert:

1. **Menschliche Autonomie und Aufsicht:** KI-Systeme sollten die Entscheidungsfreiheit und -kompetenz der Menschen unterstützen und nicht untergraben. Dies bedeutet, dass KI als Unterstützung für menschliche Entscheidungen dienen sollte und Mechanismen zur menschlichen Aufsicht und Kontrolle der Systeme implementiert werden müssen. Beispielsweise könnten Systeme entwickelt werden, die klare Hinweise geben, wann eine menschliche Überprüfung erforderlich ist, oder die es den Nutzern ermöglichen, Entscheidungen der KI-Systeme zu hinterfragen und zu überstimmen.

2. **Technische Robustheit und Sicherheit:** Zuverlässigkeit, Sicherheit und Robustheit der KI-Systeme sind entscheidend, um die Integrität und Vertrauenswürdigkeit der Technologien zu gewährleisten. Technische Robustheit erfordert die Fähigkeit der Systeme, in verschiedenen Situationen und unter unterschiedlichen Bedingungen zuverlässig zu funktionieren. Sicherheit bedeutet, dass Systeme vor Angriffen geschützt und Ausfälle minimiert werden müssen. Dies schließt auch die Notwendigkeit ein, dass KI-Systeme regelmäßig getestet und aktualisiert werden, um sicherzustellen, dass sie sicher und effektiv bleiben (European Commission, 2020a; High-Level Expert Group on Artificial Intelligence, 2019).

3. **Privatsphäre und Datenmanagement:** Der Schutz der Privatsphäre und eine verantwortungsbewusste Datenverwaltung sind essenziell. KI-Systeme müssen so gestaltet sein, dass sie die Privatsphäre der Nutzer respektieren und schützen. Dies umfasst Maßnahmen zum Datenschutz, zur Qualität und Integrität der Daten sowie zu transparenten Zugriffs- und Verarbeitungsprotokollen. Beispielsweise sollten Daten, die von KI-Systemen gesammelt werden, anonymisiert oder pseudonymisiert werden, um die Privatsphäre der Nutzer zu schützen (European Commission, 2020a).

4. **Transparenz:** Nachvollziehbarkeit, Erklärbarkeit und klare Kommunikation der KI-Systeme sind entscheidend, um Vertrauen in diese Technologien zu schaffen. Transparenz bedeutet, dass die Entscheidungen und Funktionsweisen der KI-Systeme verständlich und zugänglich erklärt werden müssen. Dies ermöglicht es den Nutzern, die Funktionsweise der Systeme zu verstehen und ihre Entscheidungen zu hinterfragen. Erklärbarkeit bezieht sich darauf, dass die Prozesse, die zu einer bestimmten Entscheidung führen, klar und nachvollziehbar sind (High-Level Expert Group on Artificial Intelligence, 2019).

5. **Vielfalt, Nichtdiskriminierung und Fairness:** KI-Systeme sollen inklusiv gestaltet sein und dürfen keine diskriminierenden Auswirkungen haben. Dies erfordert die Beseitigung von Verzerrungen in den Daten und Modellen sowie die Berücksichtigung aller Nutzergruppen. Beispielsweise sollten KI-Systeme so entwickelt werden, dass sie alle Nutzer unabhängig von deren ethnischer Herkunft, Geschlecht, Alter oder anderen persönlichen Merkmalen gleich behandeln. Dies könnte durch die Implementierung von Mechanismen zur Überprüfung und Korrektur von Verzerrungen in den Daten und Algorithmen erreicht werden (European Commission, 2020a).

6. **Gesellschaftliches und ökologisches Wohlergehen:** Die Auswirkungen von KI auf Gesellschaft und Umwelt müssen berücksichtigt werden. Dazu gehört die Förderung des gesellschaftlichen Wohlergehens und die Minimierung negativer Umweltauswirkungen. KI-Systeme sollten so gestaltet sein, dass sie positive soziale und ökologische Auswirkungen haben. Beispielsweise könnten KI-Systeme entwickelt werden, die Energieeffizienz verbessern oder zur Lösung sozialer Probleme beitragen (High-Level Expert Group on Artificial Intelligence, 2019).

7. **Rechenschaftspflicht:** Es muss klare Verantwortlichkeiten und Mechanismen zur Überprüfung und Rechenschaftspflicht geben. Dies umfasst die Möglichkeit zur Überprüfung der Systeme durch unabhängige Dritte sowie die Implementierung von Mechanismen zur Korrektur von Fehlern und zur Rechenschaftspflicht derjenigen, die die Systeme entwickeln und einsetzen. Rechenschaftspflicht bedeutet auch, dass klare Verfahren und Standards zur Überprüfung und Bewertung der KI-Systeme etabliert werden müssen (European Commission, 2020a; High-Level Expert Group on Artificial Intelligence, 2019).

### Unterschiede zwischen dem AI Act und den Anforderungen der HLEG

Die Arbeit der High-Level Expert Group on AI und der EU AI Act stellen wesentliche Schritte dar, um sicherzustellen, dass KI-Systeme im Einklang mit europäischen Werten und Grundrechten entwickelt und eingesetzt werden. Während die HLEG einen breiteren, ethisch orientierten Ansatz verfolgt, legt der EU AI Act einen stärkeren Fokus auf die Regulierung basierend auf dem Risiko, das KI-Systeme darstellen.

Ein wesentlicher Unterschied liegt in der Herangehensweise: Der EU AI Act kategorisiert KI-Systeme basierend auf dem Risiko, das sie darstellen. Es gibt verbotene Praktiken, Hochrisiko-Systeme, Systeme mit eingeschränktem Risiko und Systeme mit minimalem Risiko. Diese risikobasierte Herangehensweise bestimmt, welche Anforderungen an die jeweiligen KI-Systeme gestellt werden. Im Gegensatz dazu definiert die HLEG allgemeine Anforderungen, die für alle KI-Systeme gelten sollten, unabhängig von ihrem spezifischen Risikopotenzial (European Commission, 2020b).

Besonders hervorzuheben sind die Themen Vielfalt und ökologische Wirkung, die von der HLEG stärker betont werden als vom EU AI Act. Die HLEG legt großen Wert auf die Inklusion aller Nutzergruppen und die Vermeidung von Diskriminierung. Dies erfordert Maßnahmen zur Beseitigung von Verzerrungen in den Daten und Modellen sowie die Berücksichtigung aller demografischen Gruppen. Der EU AI Act hingegen fokussiert sich stärker auf rechtliche und technische Anforderungen und behandelt die Vielfalt weniger ausführlich (European Commission, 2020a).

Ebenso legt die HLEG besonderen Wert auf das gesellschaftliche und ökologische Wohlergehen. KI-Systeme sollten nicht nur technisch robust sein, sondern auch positive soziale und ökologische Auswirkungen haben. Dies schließt die Förderung von Energieeffizienz und die Minimierung negativer Umweltauswirkungen ein. Der EU AI Act hingegen betont stärker die Einhaltung rechtlicher Standards und die technische Sicherheit, während die ökologischen Aspekte weniger prominent behandelt werden (High-Level Expert Group on Artificial Intelligence, 2019).

### Zusammenfassung der Kriterien und Ziele der HLEG

- **Menschliche Autonomie und Aufsicht**
- **Technische Robustheit und Sicherheit**
- **Privatsphäre und Datenmanagement**
- **Transparenz**
- **Vielfalt, Nichtdiskriminierung und Fairness**
- **Gesellschaftliches und ökologisches Wohlergehen**
- **Rechenschaftspflicht**

Die Arbeit der High-Level Expert Group on AI stellt einen wichtigen Schritt dar, um sicherzustellen, dass KI-Systeme im Einklang mit europäischen Werten und Grundrechten entwickelt und eingesetzt werden. Die definierten Kriterien und Leitlinien dienen als Grundlage für die Entwicklung und Implementierung vertrauenswürdiger und ethisch verantwortungsvoller KI-Systeme. Die umfassende Betrachtung technischer, ethischer und sozialer Aspekte trägt dazu bei, dass KI nicht nur technologisch fortschrittlich, sondern auch sozial und ethisch verantwortungsvoll ist.



#### Literaturverzeichnis

European Commission. (2020a). The Ethics Guidelines for Trustworthy AI. Abgerufen von [https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai](https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai).

European Commission. (2020b). The EU AI Act: Regulatory framework proposal for Artificial Intelligence. Abgerufen von [https://digital-strategy.ec.europa.eu/en/library/proposal-regulation-l