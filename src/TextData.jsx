export const TextData = {
    Einleitung: {
        VideoLink: "https://www.youtube-nocookie.com/embed/kxsbAi3gmeE?si=PL74OEuieI7W2hce&amp;rel=0&amp;modestbranding=1",
        Texte: [
            { title: "EU AI Act", texts: [
                "## Ziele des EU AI Acts:",
                "Mit dem Eu AI Act haben es sich die Mitgliedsstaaten der EU zum Ziel gesetzt das erste umfassende Regularium für den Umgang mit KI basierten Systemen zu verfassen. Ziel ist es rechtliche Rahmenbedingungen zu schaffen in denen KI Projekte in Wirtschaft, Forschung und Gesellschaft so eingesetzt und entwickelt werden können, dass ein vertrauenswürdiger Umgang mit den Systemen möglich ist, die die fundamentalen Rechte der Nutzenden, aber auch Aspekte wie Sicherheit und das Handeln auf Grundlage ethischer Prinzipien mit einbeziehen. Dabei soll der EU AI Act eine über die Grenzen der EU hinausgehenden Impuls setzen und ähnlich wie die DSGVO im Datenschutz einen Standard stellen, der auch in nicht EU Ländern wie den USA oder Japan genutzt wird.",
                "",
                "## Entwickluing des Acts:",
                "Bei der Entwicklung des EU AI Act handelt es sich um einen komplexen bürokratischen Prozess, der sich nicht nur über viel Jahre hinweg entwickelt, sondern auch große Teile des parlamentarischen EU Apparats durchlaufen hat.",
                "##list Der erste Schritt dieses Prozesses war 2021 ein erster Vorschlag zur Regulierung von künstlicher Intelligenz durch die EU Kommission. ##list Im November 2021 entsteht ein erster Kompromisstext, der u.A. gesamtgesellschaftlich relevante Anwendungsfälle wie Social Scoring Systeme oder biometrische Erkennungssystem beinhaltet. ##list In 2022 wird dieser Kompromisstext in den Gremien diskutiert und  Stück für Stück erweitert und ausgearbeitet. So kamen das Thema der Transparentpflicht (Februar) oder die Regulierung von Systemen mit allgemeinen Verwendungszwecken (Mai) nach und nach hinzu. Auch Fragen der Haftbarkeit und der Durchsetzung von Schadensansprüchen werden in dieser Zeit diskutiert. ##list Am ersten Juni 2022 endet die Frist für die Fraktionen des EU Parlamentes für die Einreichung von Änderungsanträgen des EU AI Gesetzes. Insgesamt sind mehrere Tausend Anträge und Änderungen eingereicht, die in die Ausarbeitung des finalen Textes einbezogen worden sind. ##list Am 14. Juni 2023 kommt es dann zur großen Abstimmung im EU Parlament. Mit 499 Ja-Stimmen, 28 Nein-Stimmen und 93 Enthaltungen wird eine Verhandlungsposition zum AI Gesetz angenommen. ##list Am 09. Dezember 2023 erzielen Rat und Parlament dann eine vorläufige Einigung zum AI Gesetzt. ##list Mit Beginn von 2024 wird das Europäische Büro für künstliche Intelligenz innerhalb der Kommission eingerichtet und weitere Prozesse für die schrittweise Umsetzung und Ausgestaltung des EU AI Acts angestoßen.",
                "Ein vollständige Übersicht des zeitlichen Verlaufs des Gestaltungsprozesses und der damit verbundenen Institutionen und Zwischenstände finden Sie hier: https://artificialintelligenceact.eu/de/entwicklungen/.",
                "",
                "## Vergleich mit anderen Ländern / Regionen:",
                "Betrachtet man den EU AI Act als groß angelegte Normierung innerhalb des europäischen Raums, drängt sich schnell die Frage auf welche Auswirkungen die Gesetzgebung außerhalb seiner Mitgliedsstaaten haben wird. Es kann dabei davon ausgegangen werden, dass das EU Parlament zwar primär die Regulierung im eigenen Legislaturbereich im Blick hat, aber auch auf andere große Volkswirtschaften wie die USA, China und das Vereinigte Königreich schaut, wenn es um den Einsatz von KI geht. Wie schon bei der Datenschutz Grundverordnung (DSGVO) scheint hier der Gedanke einen weitreichenden \"Goldstandard\" zu schaffen der auch die Gesetzgebung in den Nationen außerhalb der EU bestimmt. Wir halten es deshalb für sinnvoll einen kurzen Blick auf den aktuellen Stand der KI Regulierung in den anderen Nationen zu werfen, um diese Vorstellung besser einordnen zu können. Schaut man dazu beispielsweise über den Kanal ins Vereinigte Königreich so stellt man fest, dass auch dort weitreichende Maßnahmen für die Regulierung und den ethischen Umgang mit KI bereits getroffen worden sind. Die britische Regierung setzt dabei auf bestehende sektorale Vorgaben wie bspw. die KI Prinzipien der OECD oder die Empfehlung zu ehtischem Umgang mit KI der UNESCO. Diese Übergeordneten Richtlinien werden durch lokal angetriebene Maßnahmen erweitert. Von besonderer Bedeutung ist dabei zum Beispiel die Bletchley Declaration aus November 2023 bei der sich 28 Länder, darunter die Vereinigten Staaten, China und die Europäische Union geeinigt haben international bei der Bewältigung von Herausforderungen und Risiken im Bereich der KI zusammenzuarbeiten. Im Fokus standen dabei vor allem \"frontier\", Systeme, also KI Grundlagenmodelle, die für alle möglichen Anwendungsfälle nutzbar gemacht werden können so wie bspw. die ChatGPT zugrundeliegenden LLMs.",
            ]}
        ]
    },
    Risikostufen: {
        VideoLink: "https://www.youtube-nocookie.com/embed/zkfqjX6om8g?si=XKEX_r1jRmViUJkB&amp;rel=0&amp;modestbranding=1",
        Texte: [
            { title: "Wie funktionieren Risikostufen", texts: [
                "Wie zuvor besprochen wollen wir uns als erstes mit dem Thema Risikostufen beschäftigen. Zentral ist dabei das Verständnis, dass nicht jedes KI-System gleich viel Einfluss auf uns als Menschen hat - bei manchen sind die Risiken höher als bei anderen. Manchmal sogar inakzeptabel hoch. Die Notwendigkeit einer solchen Unterteilung hat auch die EU erkannt und deswegen ein zentrales Werkzeug im EU AI Act erschaffen - die Einteilung von KI-Systemen in vier Risikostufen:",
                "Die niedrigste Stufe geht davon aus, dass durch ein KI-System nur ein minimales Risiko besteht. Beispiele hierfür sind KI-Systeme, die in Videospielen eingesetzt werden oder intelligente Spam-Filter, die bestenfalls nach festen Regeln vorgehen.",
                "Die zweite Stufe kategorisiert Systeme mit einem begrenzten Risiko s.g. limited risk. Bei diesen gelten insbesondere Transparenzvorschriften, d.h., es muss klar gemacht werden, dass KI Systeme eingesetzt werden und wie. Das gilt z.B. beim Einsatz von Chatbots.",
                "Das Herzstück des Acts stellen final die High-Risk-Systeme da. Hierzu zählen alle Systeme, die irgendeiner Zulassung bedürfen. Dazu gehören medizinische Systeme, Systeme im Bereich der Strafverfolgung oder auch Systeme, die zur politischen Meinungsbildung geeignet sind. Der EU AI Act formuliert in dieser Kategorie hohe Anforderungen in Bezug auf die verschiedenen genutzten Variablen. So beispielsweise für die Dokumentation des Trainings, des genutzten Models und der bei der Entwicklung getroffenen Designentscheidungen. Für uns ist dabei vor allem der Punkt Transparenz und die Möglichkeit, der menschlichen Kontrollfähigkeit interessant."
            ]},
            { title: "Zu beachten bei Risikostufen", texts: [
                "Letztlich gibt es noch Systeme mit einem inakzetabel hohen Risiko. Darunter fallen alle Systeme, die für Menschen und unsere demokratischen Werte gefährlich sein können bspw. Social Scoring Systeme, Deep Fake Tools oder Profiling systeme. Alle Systeme, die in diese Kategorien fallen, sind vollständig verboten. Generell steht noch nicht fest, wie viele Systeme am Ende im Bereich high risk landen werden, aber wir gehen derzeit davon aus, dass es zwischen 15-30% sein könnten. Insbesondere gemeinwohlorienterte KI kann zudem das ziel haben, allen Anforderungen gerecht zu werden - egal ob high risk oder nicht.",
                "Ein letzter Hinweis noch: sehr mächtige KI-Modelle, die man für viele Zwecke einsetzen kann, nennt die Verordnung \"General Purpose AI\". Dazu gehören so Systeme wie große Sprachmodelle, die hinter ChatGPT stecken. Diese Systeme gehören nicht explizit einer Risikostufe an, haben aber nochmal ganz eigene Regeln, die in einem gesonderten Kapitel des EU AI Acts festgehalten sind.",
            ]},
        ]
    },
    Designimplikationen: {
        VideoLink: "https://www.youtube-nocookie.com/embed/0rPt4Grl8D4?si=2BKL-W0acY6LRGPu&rel=0&modestbranding=1",
        Texte: [
            { title: "Weitergehende Kriterien", texts: [
                'Nachdem wir uns nun damit beschäftigt haben was der EU AI Act eigentlich ist und wie er versucht durch Risikostufen die Nutzung von verschiedenen Arten von KI Systemen zu regulieren, wollen wir uns jetzt einmal angucken was diese Regeln jetzt für die Gestaltung bedeuten. Schauen wir uns das mal an ein paar Beispielen an! Als erstes greifen wir unseren Telefon-Seelsorge Chatbot auf. Egal ob er am Ende in einem Kontext mit hohem Risiko oder nicht eingesetzt wird, es MUSS klar gemacht werden, dass hier kein Mensch agiert. Insofern sollte man z.B. nicht "schreibt" als Animation einsetzen, wenn der Bot antwortet. Das wäre täuschend und deutet auf einen wirklichen Menschen hin. Eine Alternative wäre der Einsatz von eher technischen Begriffen wie "Antwort wird berechnet", die auf die maschinelle Natur des Gesprächspartners hinweisen. Auch bei einer gegebenen Antwort sollte der Chatbot seine Grenzen verdeutlichen, z.B. indem er markiert, welche Absichten (Intent) er erkannt hat. Zusätzlich muss die menschliche Kontrollfähigkeit hoch genug sein - Mitarbeitende der Telefonseelsorge müssen den Chatbot (leicht) anpassen können. Es muss ein Interface geben, um z.B. die Länge der Antworten zu steuern oder wann der Chatbot an menschliche Agenten weitergibt so, dass er möglichst einfach und barrierefrei in der praktischen Alltagsnutzung ist. Greifen wir uns als zweites Beispiel den Algorithmus zur Detektion von Hassnachrichten. Bei einer solchen Anwendung geht es uns vor allem darum, zu prüfen, ob er so arbeitet, wie von uns gewünscht. Daher sollten wir bei der Gestaltung daran denken, dass er nachvollziehbare Erklärungen über sein trainiertes Modell abgeben kann. Eine Möglichkeit wären die von uns erklärten Shapley Values aus dem Modul X. *Hier nochmal ein Beispiel.* Nutzende können dann die Visualisierung aufrufen und für eine bestimmte Nachricht erkennen, weshalb diese als Hatespeech klassifiziert wird. alternativ kann man aber auch über so genannte *counterfactual explanation* arbeiten. Bei diesen kann die nutzende Person sich anzeigen lassen, durch welche Wortänderungen das System NICHT mehr von *hate speech* ausgehen würde. Hier ist jedoch Vorsicht geboten - die Systeme können so vielleicht leichter ausgetrickst werden, weil man genau sehen kann, wie sie funktionieren. Es ist also eine Gewissensentscheidung wie transparent die Antworten des Systems für die Nutzenden sein sollen. Unser letztes Beispiel beschäftigt sich mit der Empfehlung von Nahrungsmitteln. Bei einem solchen System ist es unser Ziel zu verhindern, dass Menschen von Handlungen/ Empfehlungen überzeugt werden, die sie selbst nicht wollten oder gutheißen. Daher sollten wir explizit zeigen, weshalb ein Vorschlag für ein spezifisches Lebensmittel gegeben wird, z.B. das stärkste Feature wie (Beispiel) für einen Vorschlag hervorheben. Eine weiter Möglichkeit, um ungewollte Beeinflussung der Nutzenden zu verhindern ist es die Distanz zwischen präsentierten Vorschlägen zu kommunizieren bspw. wenn sich zwei Produkte, die durch das System auf verschiedene Plätze eingeordnet worden sind eigentlich nur sehr geringfügig unterscheiden. Mit diesen Anwendungsbeispielen im Kopf, was bedeutet das für Sie? Welche Designentscheidungen müssen sie treffen so, dass ihr System nicht nur konform mit dem EU AI ACT ist, sondern auch die Bedürfnisse der Nutzenden in den Mittelpunkt stellt.',
            ]}
        ]
      },
      Fazit: {
        VideoLink: "https://www.youtube-nocookie.com/embed/PZAkt-EuKn0?si=g-hnbptFXBW6kajx&amp;rel=0&amp;modestbranding=1",
        Texte: [
            { title: "Diskussion", texts: [
                "Lorem ipsum, dolor sit amet consectetur adipisicing elit. Quos temporibus itaque sit ipsa ex reiciendis, sint earum hic aliquid vel atque enim, inventore, magni numquam commodi iste? Quam, ab iure. Lorem ipsum dolor sit amet consectetur adipisicing elit. Consequatur odit aut impedit facilis laboriosam, adipisci eaque eos incidunt ad tenetur debitis repudiandae eius non recusandae. Quo harum at nostrum veritatis.",
            ]}
        ]
      },
}